{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/linafaik/Documents/projects/knowledge_graph_llm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linafaik/Documents/projects/knowledge_graph_llm/py_env/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_storage = \"storage/dynamic_llm\"\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graph loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# Load the index from the storage context\n",
    "# StorageContext.from_defaults() initializes a default storage context with a specified directory for persistent storage\n",
    "index = load_index_from_storage(\n",
    "    StorageContext.from_defaults(persist_dir=path_storage)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vector context retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The International Consortium of Investigative Journalists (ICIJ) helped organize the research and document review once SÃ¼ddeutsche Zeitung realized the scale of the work required to validate the authenticity of the leaked data. The ICIJ enlisted reporters and resources from various media outlets to investigate individuals and organizations associated with Mossack Fonseca. Additionally, the ICIJ released the leaked documents from the Panama Papers scandal on its website after verifying the source and content.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from the llama_index library\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Import the VectorContextRetriever class from the property_graph module\n",
    "from llama_index.core.indices.property_graph import VectorContextRetriever\n",
    "\n",
    "# Create a sub-retriever using VectorContextRetriever\n",
    "# This will use the property graph store and vector store from the loaded index\n",
    "# The embed_model parameter specifies the model to be used for embedding queries (e.g., OpenAI's embedding model)\n",
    "sub_retriever = VectorContextRetriever(\n",
    "  index.property_graph_store, \n",
    "  vector_store=index.vector_store,\n",
    "  embed_model=OpenAIEmbedding(model_name=EMBEDDING_MODEL),\n",
    ")\n",
    "\n",
    "# Create a retriever from the index using the previously defined sub-retriever\n",
    "retriever = index.as_retriever(sub_retrievers=[sub_retriever])\n",
    "\n",
    "# Initialize the query engine using the retriever\n",
    "# The query engine will use the retriever(s) to process and return responses to queries\n",
    "query_engine = index.as_query_engine(\n",
    "    sub_retrievers=[retriever]\n",
    ")\n",
    "\n",
    "# Perform a query on the query engine to retrieve information about ICIJ's involvement in the Panama Papers scandal\n",
    "print(\n",
    "    query_engine.query(\n",
    "    \"How was the International Consortium of Investigative Journalists (ICIJ) involved in the Panama Papers scandal?\"\n",
    "    ).response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM synonym retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from the llama_index library\n",
    "from llama_index.core.indices.property_graph import LLMSynonymRetriever\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Create a sub-retriever using LLMSynonymRetriever\n",
    "# This retriever will utilize a language model (LLM) to retrieve paths with synonymous terms or related concepts\n",
    "# from the property graph store within the index\n",
    "sub_retriever = LLMSynonymRetriever(\n",
    "    index.property_graph_store,  # The property graph store from the index is used as the data source\n",
    "    llm=OpenAI(model=LLM_MODEL, temperature=TEMPERATURE),  # Initialize the LLM with specified model and temperature\n",
    "    include_text=True,  # Include the source chunk text in the retrieved paths\n",
    "    max_keywords=100,  # Maximum number of keywords to be considered for retrieval\n",
    "    path_depth=5,  # Limit the depth of the search paths to 5 levels\n",
    ")\n",
    "\n",
    "# Create a retriever from the index using the previously defined sub-retriever\n",
    "retriever = index.as_retriever(sub_retrievers=[sub_retriever])\n",
    "\n",
    "# Initialize the query engine using the retriever\n",
    "# The query engine will use the retriever(s) to process and return responses to queries\n",
    "query_engine = index.as_query_engine(\n",
    "    sub_retrievers=[retriever]\n",
    ")\n",
    "\n",
    "# Perform a query on the query engine to retrieve information about ICIJ's involvement in the Panama Papers scandal\n",
    "# The response is then printed to the console\n",
    "print(\n",
    "    query_engine.query(\n",
    "    \"Who were the main people involved in Panama Papers scandal?\"\n",
    "    ).response\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
