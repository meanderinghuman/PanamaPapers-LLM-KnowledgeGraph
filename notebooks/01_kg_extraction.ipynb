{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/linafaik/Documents/projects/knowledge_graph_llm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linafaik/Documents/projects/knowledge_graph_llm/py_env/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_text  = \"data/panama_papers\"\n",
    "path_output_storage = \"storage\"\n",
    "path_output = \"outputs\"\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL =\"text-embedding-3-small\"\n",
    "TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_output_storage):\n",
    "    os.makedirs(path_output_storage)\n",
    "\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(path_input_text).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Schema-based extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linafaik/Documents/projects/knowledge_graph_llm/py_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 18/18 [00:00<00:00, 1557.36it/s]\n",
      "Extracting paths from text with schema: 100%|██████████| 18/18 [00:25<00:00,  1.39s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:01<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define a name for the keyword extractor\n",
    "kw_extractor_name = \"schema_llm\"\n",
    "\n",
    "# Define the possible entity types for the knowledge graph\n",
    "entities = Literal[\"PERSON\", \"COMPANY\", \"COUNTRY\", \"BANK\", \"SCANDAL\"]\n",
    "\n",
    "# Define the possible relations between the entities in the knowledge graph\n",
    "relations = Literal[\"OWNS\", \"LOCATED_IN\", \"INVOLVED_IN\"]\n",
    "\n",
    "# Define the schema that outlines which entities can have which relations\n",
    "schema = {\n",
    "    \"PERSON\": [\"OWNS\", \"LOCATED_IN\", \"INVOLVED_IN\"],\n",
    "    \"COMPANY\": [\"OWNS\", \"LOCATED_IN\", \"INVOLVED_IN\"],\n",
    "    \"COUNTRY\": [\"LOCATED_IN\"],\n",
    "    \"BANK\": [\"LOCATED_IN\", \"INVOLVED_IN\"],\n",
    "    \"SCANDAL\": [\"INVOLVED_IN\"],\n",
    "}\n",
    "\n",
    "# Create an instance of SchemaLLMPathExtractor to extract paths based on the defined schema\n",
    "kg_extractor = SchemaLLMPathExtractor(\n",
    "  llm=OpenAI(model=LLM_MODEL, temperature=TEMPERATURE),  # Use OpenAI's language model with the specified parameters\n",
    "  possible_entities=entities,  # Define the types of entities to extract\n",
    "  possible_relations=relations,  # Define the types of relations to extract\n",
    "  kg_validation_schema=schema,  # Use the predefined schema for validation\n",
    "  strict=True,  # Enforce strict validation; only entities and relations defined in the schema will be allowed\n",
    ")\n",
    "\n",
    "# Create a PropertyGraphIndex from the provided documents, using the specified embedding model\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,  # The input documents to be processed and indexed\n",
    "    embed_model=OpenAIEmbedding(model_name=EMBEDDING_MODEL),  # Use OpenAI's embedding model for document representation\n",
    "    show_progress=True,  # Display progress during indexing\n",
    "    kg_extractors=[kg_extractor],  # Use the previously defined SchemaLLMPathExtractor for extracting knowledge graph paths\n",
    ")\n",
    "\n",
    "# Define the storage path for the keyword extractor\n",
    "path_output_storage_kg_extractor = f\"{path_output_storage}/{kw_extractor_name}/\"\n",
    "\n",
    "# Create the storage directory if it doesn't already exist\n",
    "if not os.path.exists(path_output_storage_kg_extractor):\n",
    "    os.makedirs(path_output_storage_kg_extractor)\n",
    "\n",
    "# Persist the index's storage context to the specified directory\n",
    "index.storage_context.persist(persist_dir=path_output_storage_kg_extractor)\n",
    "\n",
    "# Save the knowledge graph as a NetworkX graph to an HTML file\n",
    "index.property_graph_store.save_networkx_graph(name=f\"{path_output}/kg_{kw_extractor_name}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Free-form extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 18/18 [00:00<00:00, 832.76it/s]\n",
      "Extracting paths from text: 100%|██████████| 18/18 [00:14<00:00,  1.27it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:01<00:00,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define a name for the keyword extractor\n",
    "kw_extractor_name = \"free_form\"\n",
    "\n",
    "# Create an instance of SimpleLLMPathExtractor\n",
    "kg_extractor = SimpleLLMPathExtractor(\n",
    "    llm=OpenAI(model=LLM_MODEL, temperature=TEMPERATURE)\n",
    "    )\n",
    "\n",
    "# Create a PropertyGraphIndex from the provided documents, using the specified embedding model\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,  # The input documents to be processed and indexed\n",
    "    embed_model=OpenAIEmbedding(model_name=EMBEDDING_MODEL),  # Use OpenAI's embedding model for document representation\n",
    "    show_progress=True,  # Display progress during indexing\n",
    "    kg_extractors=[kg_extractor],  # Use the previously defined SchemaLLMPathExtractor for extracting knowledge graph paths\n",
    ")\n",
    "\n",
    "# Define the storage path for the keyword extractor\n",
    "path_output_storage_kg_extractor = f\"{path_output_storage}/{kw_extractor_name}/\"\n",
    "\n",
    "# Create the storage directory if it doesn't already exist\n",
    "if not os.path.exists(path_output_storage_kg_extractor):\n",
    "    os.makedirs(path_output_storage_kg_extractor)\n",
    "\n",
    "# Persist the index's storage context to the specified directory\n",
    "index.storage_context.persist(persist_dir=path_output_storage_kg_extractor)\n",
    "\n",
    "# Save the knowledge graph as a NetworkX graph to an HTML file\n",
    "index.property_graph_store.save_networkx_graph(name=f\"{path_output}/kg_{kw_extractor_name}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 18/18 [00:00<00:00, 1805.94it/s]\n",
      "Extracting and inferring knowledge graph from text: 100%|██████████| 18/18 [00:33<00:00,  1.84s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:01<00:00,  3.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.property_graph import DynamicLLMPathExtractor\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define a name for the keyword extractor\n",
    "kw_extractor_name = \"dynamic_llm\"\n",
    "\n",
    "# Define the possible entity types for the knowledge graph\n",
    "entities = [\"PERSON\", \"COMPANY\", \"COUNTRY\", \"BANK\", \"SCANDAL\"]\n",
    "\n",
    "# Define the possible relations between the entities in the knowledge graph\n",
    "relations = [\"OWNS\", \"LOCATED_IN\", \"INVOLVED_IN\"]\n",
    "\n",
    "# Create an instance of SimpleLLMPathExtractor\n",
    "kg_extractor = DynamicLLMPathExtractor(\n",
    "    llm=OpenAI(model=LLM_MODEL, temperature=TEMPERATURE),\n",
    "    allowed_entity_types=entities,\n",
    "    allowed_relation_types=relations,\n",
    "    )\n",
    "\n",
    "# Create a PropertyGraphIndex from the provided documents, using the specified embedding model\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,  # The input documents to be processed and indexed\n",
    "    embed_model=OpenAIEmbedding(model_name=EMBEDDING_MODEL),  # Use OpenAI's embedding model for document representation\n",
    "    show_progress=True,  # Display progress during indexing\n",
    "    kg_extractors=[kg_extractor],  # Use the previously defined SchemaLLMPathExtractor for extracting knowledge graph paths\n",
    ")\n",
    "\n",
    "# Define the storage path for the keyword extractor\n",
    "path_output_storage_kg_extractor = f\"{path_output_storage}/{kw_extractor_name}/\"\n",
    "\n",
    "# Create the storage directory if it doesn't already exist\n",
    "if not os.path.exists(path_output_storage_kg_extractor):\n",
    "    os.makedirs(path_output_storage_kg_extractor)\n",
    "\n",
    "# Persist the index's storage context to the specified directory\n",
    "index.storage_context.persist(persist_dir=path_output_storage_kg_extractor)\n",
    "\n",
    "# Save the knowledge graph as a NetworkX graph to an HTML file\n",
    "index.property_graph_store.save_networkx_graph(name=f\"{path_output}/kg_{kw_extractor_name}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implicit extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 18/18 [00:00<00:00, 1673.33it/s]\n",
      "Extracting implicit paths: 100%|██████████| 18/18 [00:00<00:00, 60397.98it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.property_graph import ImplicitPathExtractor\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define a name for the keyword extractor\n",
    "kw_extractor_name = \"implicit\"\n",
    "\n",
    "# Create an instance of SimpleLLMPathExtractor\n",
    "kg_extractor = ImplicitPathExtractor()\n",
    "\n",
    "# Create a PropertyGraphIndex from the provided documents, using the specified embedding model\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,  # The input documents to be processed and indexed\n",
    "    embed_model=OpenAIEmbedding(model_name=EMBEDDING_MODEL),  # Use OpenAI's embedding model for document representation\n",
    "    show_progress=True,  # Display progress during indexing\n",
    "    kg_extractors=[kg_extractor],  # Use the previously defined SchemaLLMPathExtractor for extracting knowledge graph paths\n",
    ")\n",
    "\n",
    "# Define the storage path for the keyword extractor\n",
    "path_output_storage_kg_extractor = f\"{path_output_storage}/{kw_extractor_name}/\"\n",
    "\n",
    "# Create the storage directory if it doesn't already exist\n",
    "if not os.path.exists(path_output_storage_kg_extractor):\n",
    "    os.makedirs(path_output_storage_kg_extractor)\n",
    "\n",
    "# Persist the index's storage context to the specified directory\n",
    "index.storage_context.persist(persist_dir=path_output_storage_kg_extractor)\n",
    "\n",
    "# Save the knowledge graph as a NetworkX graph to an HTML file\n",
    "index.property_graph_store.save_networkx_graph(name=f\"{path_output}/kg_{kw_extractor_name}.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
